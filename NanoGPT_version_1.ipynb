{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0x/so/CVcHbGN3b9dmn3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phongvu009/nano_gpt/blob/main/NanoGPT_version_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L3u342zFha0N",
        "outputId": "9ef1b2bd-46e7-4fca-af4c-4417059afb3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "lRUg4-1xhicF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "BwOnvoN5hvDK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper Parameters"
      ],
      "metadata": {
        "id": "v4dgsPr40iBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 64\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "xlybGgJU0lyP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000 # for training"
      ],
      "metadata": {
        "id": "87TMGAzw4uov"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_point = 100\n",
        "eval_iters = 200 # for evaluation"
      ],
      "metadata": {
        "id": "JFIXAieq8vOv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "qsQPKU0BIZIK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8KfPqjjJD19"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "8hw9W4OLh2Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA0JkNshh3_r",
        "outputId": "6f7fbce7-c2cf-4441-a023-5667c4ad8638"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-07 22:15:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-12-07 22:15:05 (23.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0vyCK_qh8_B",
        "outputId": "6053bb38-968d-453f-9c40-7bdcd05243d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open file\n",
        "with open('input.txt', mode='r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "vyGuXZzKiOwm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS0fGTc7igxV",
        "outputId": "3cb681d7-135d-4c52-cca4-44e7a54b880c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "print(len(chars))\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgYGVPCOik0R",
        "outputId": "8bda517e-93d2-45dc-d346-be7af7fb19d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create mapping characters to index\n",
        "stoi = { char:idx for idx,char in enumerate(chars) }\n",
        "\n",
        "#Create mapping index to characters\n",
        "itos = { idx:char for idx,char in enumerate(chars) }\n",
        "\n",
        "#encode\n",
        "encode = lambda x :  [ stoi[ix] for ix in x ]\n",
        "\n",
        "#decode\n",
        "decode = lambda x : ''.join([ itos[ix] for ix in x])\n",
        "\n",
        "print(encode('this is a test !'))\n",
        "print(decode(encode('this is a test !')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo-HvoiEyApY",
        "outputId": "f49fb475-0864-4225-d574-595706ed8a5f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58, 46, 47, 57, 1, 47, 57, 1, 39, 1, 58, 43, 57, 58, 1, 2]\n",
            "this is a test !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encode data and transform into tensor\n",
        "data = torch.tensor(encode(text))\n",
        "print(data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JKhxEB8y4k-",
        "outputId": "04b3a693-5e86-4981-a8f9-1b6c98e823b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split data\n",
        "n = int(0.9 * len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "dyT_SD60zhCF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get a random batch of sample\n",
        "def get_batch(data_stage):\n",
        "  data = train_data if data_stage == 'train' else val_data\n",
        "  #get random ix\n",
        "  idx = torch.randint(0, len(data) - seq_len, (batch_size,))\n",
        "  xb = torch.stack([ data[i:i+seq_len] for i in idx ]).to(device)\n",
        "  yb = torch.stack ( [data[i+1:i+1+seq_len] for i in idx] ).to(device)\n",
        "\n",
        "  return xb,yb\n"
      ],
      "metadata": {
        "id": "n9UkthB2ztjl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb,yb = get_batch('train')\n",
        "print(xb.shape)\n",
        "# print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjA5Z5hq16vt",
        "outputId": "cfcb4973-0d88-4945-c1dd-a2b4689ea617"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "a-Q9pWTW3zHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars) # 65 characters"
      ],
      "metadata": {
        "id": "p_riBHBM2NZL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttn(nn.Module):\n",
        "  def __init__(self,d_model:int , n_head:int, dropout:float ):\n",
        "    super().__init__()\n",
        "    # head dims as d_k = d_model // n_head\n",
        "    self.d_model = d_model\n",
        "    self.n_head = n_head\n",
        "    #initialize weights for query, key , value\n",
        "    self.w_attn = nn.Linear(d_model, 3*d_model, bias =True)\n",
        "    #projection\n",
        "    self.proj = nn.Linear(d_model, d_model)\n",
        "    #regularization\n",
        "    self.att_dropout = nn.Dropout(dropout)\n",
        "    self.resid_dropout = nn.Dropout(dropout)\n",
        "    #define mask\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len,seq_len).\n",
        "                                            view(1,1, seq_len, seq_len)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x input shape (batch_size, seq_len, d_model)\n",
        "    B,T,C = x.shape\n",
        "    #mapping x to query, key , value\n",
        "    q,k,v = self.w_attn(x).split(self.d_model, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, head, seq_len, d_k)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, head, seq_len, d_k)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, head, seq_len ,d_k)\n",
        "    #attention scores\n",
        "    d_k = k.size(-1)\n",
        "    att = q @ k.transpose(-2,-1) / math.sqrt(d_k) # (B, head, seq_len, seq_len)\n",
        "    #apply mask\n",
        "    att = att.masked_fill( self.tril[:,:,:T,:T] == 0 , float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    att = self.att_dropout(att)\n",
        "    att_value = att @ v # (B, head, seq_len , d_model)\n",
        "    #concat all heads to one\n",
        "    att_value = att_value.transpose(1,2).contiguous().view(B,T,C) # C = head * (C/head)\n",
        "    #projection layer\n",
        "    out = self.proj(self.resid_dropout(att_value))\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "F5mlNLKJ6A0J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model:int, dropout:float):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(d_model, 4*d_model),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*d_model, d_model),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    #input x shape as (batch_size, seq_len , d_model)\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "5WRHZ-gO8rUM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features:int, eps:float=1e-6) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.gama = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    #input x shape as (batch_size, seq_len, d_model)\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    y = ( x - mean) / (std + self.eps)\n",
        "    return self.gama * y + self.beta"
      ],
      "metadata": {
        "id": "ip8nIGhV9g1F"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, d_model:int, n_head:int , dropout:float):\n",
        "    super().__init__()\n",
        "    assert d_model % n_head == 0 , \"can not divide into heads\"\n",
        "    #layerNorm\n",
        "    self.ln_1 = LayerNorm(d_model)\n",
        "    #Attention\n",
        "    self.sa = MultiHeadAttn(d_model, n_head , dropout)\n",
        "    #layerNorm\n",
        "    self.ln_2 = LayerNorm(d_model)\n",
        "    #mlp\n",
        "    self.ffw = FeedForward(d_model, dropout)\n",
        "\n",
        "  def forward(self,x ):\n",
        "    # input x shape : (batch_size, seq_len, d_model)\n",
        "    # Add layer norm and skip connection\n",
        "    x = x + self.sa(self.ln_1(x))\n",
        "    x = x + self.ffw(self.ln_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "UtIwCIPI5Jfz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoGPTModel(nn.Module):\n",
        "  def __init__(self, vocab_size:int, d_model:int, n_head:int , n_layer:int, dropout:float=0.0):\n",
        "    super().__init__()\n",
        "    #Embedding Layer\n",
        "    self.embedding_table = nn.Embedding(vocab_size, d_model)\n",
        "    #position\n",
        "    self.pos_embedding_table = nn.Embedding(seq_len, d_model)\n",
        "    # Attention Block\n",
        "    self.blocks = nn.Sequential( *[Block(d_model, n_head, dropout) for _ in range(n_layer)])\n",
        "    # Final layer norm\n",
        "    self.ln_f = LayerNorm(d_model)\n",
        "    #out\n",
        "    self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x, targets = None):\n",
        "    #input x shape as (batch_size, seq_len)\n",
        "    B,T = x.shape\n",
        "\n",
        "    #Embedding\n",
        "    emb_token = self.embedding_table(x) #(B,T,d_model)\n",
        "    #position\n",
        "    pos_token = self.pos_embedding_table( torch.arange(T,device=device)) # (seq_len , d_model)\n",
        "    x = emb_token + pos_token\n",
        "    #attention block\n",
        "    x = self.blocks(x)\n",
        "    # apply layer norm\n",
        "    x = self.ln_f(x) # (B, T , C)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size)\n",
        "    if targets is None: # none training\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C) # get 2-d matrix\n",
        "      targets = targets.view(B*T) # flat tensor\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens=200):\n",
        "    #idx : long array (B,T)\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-seq_len:] # get the seq_len from right to left\n",
        "      #get predictions\n",
        "      logits,loss = self(idx_cond)\n",
        "      #get the last one\n",
        "      logits = logits[:,-1,:] # (B,T,C) -> (B,C)\n",
        "      #apply softmax to get probability\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      #sample from distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "      #add new token to current sequence\n",
        "      idx = torch.cat( (idx,idx_next),dim=1)\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "BdPpUKVY37UF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "7fyCm72X66j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NanoGPTModel(vocab_size, d_model, n_head, n_layer).to(device)\n",
        "print(f\" Model Parameters - { sum( p.numel() for p in model.parameters()) / 1e6}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg0HxLhP7JEF",
        "outputId": "9431148e-0cd3-4850-be5a-b708681923f7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model Parameters - 10.722113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "afKtFuNP7-Ft"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimate loss\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for data_stage in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for i in range(eval_iters):\n",
        "      xb,yb = get_batch(data_stage)\n",
        "      logits, loss = model(xb,yb)\n",
        "      losses[i] = loss.item()\n",
        "    out[data_stage] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "uh2Dgjc48Z4K"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter % check_point == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"At {iter}/{max_iters}: train loss is {losses['train']:.4f} , val loss is {losses['val']:.4f}\")\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB1Aq1URAztj",
        "outputId": "4e751864-fdfb-4cc4-c7d3-c1a4160f474e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At 0/5000: train loss is 4.3360 , val loss is 4.3306\n",
            "At 100/5000: train loss is 2.3493 , val loss is 2.3634\n",
            "At 200/5000: train loss is 2.0799 , val loss is 2.1225\n",
            "At 300/5000: train loss is 1.8916 , val loss is 1.9828\n",
            "At 400/5000: train loss is 1.7718 , val loss is 1.8887\n",
            "At 500/5000: train loss is 1.6871 , val loss is 1.8255\n",
            "At 600/5000: train loss is 1.6240 , val loss is 1.7860\n",
            "At 700/5000: train loss is 1.5736 , val loss is 1.7485\n",
            "At 800/5000: train loss is 1.5405 , val loss is 1.7117\n",
            "At 900/5000: train loss is 1.5060 , val loss is 1.6934\n",
            "At 1000/5000: train loss is 1.4823 , val loss is 1.6744\n",
            "At 1100/5000: train loss is 1.4555 , val loss is 1.6492\n",
            "At 1200/5000: train loss is 1.4436 , val loss is 1.6426\n",
            "At 1300/5000: train loss is 1.4220 , val loss is 1.6275\n",
            "At 1400/5000: train loss is 1.4014 , val loss is 1.6084\n",
            "At 1500/5000: train loss is 1.3885 , val loss is 1.6049\n",
            "At 1600/5000: train loss is 1.3795 , val loss is 1.5937\n",
            "At 1700/5000: train loss is 1.3646 , val loss is 1.5853\n",
            "At 1800/5000: train loss is 1.3519 , val loss is 1.5846\n",
            "At 1900/5000: train loss is 1.3421 , val loss is 1.5817\n",
            "At 2000/5000: train loss is 1.3325 , val loss is 1.5711\n",
            "At 2100/5000: train loss is 1.3180 , val loss is 1.5724\n",
            "At 2200/5000: train loss is 1.3093 , val loss is 1.5672\n",
            "At 2300/5000: train loss is 1.3039 , val loss is 1.5693\n",
            "At 2400/5000: train loss is 1.2950 , val loss is 1.5655\n",
            "At 2500/5000: train loss is 1.2843 , val loss is 1.5533\n",
            "At 2600/5000: train loss is 1.2813 , val loss is 1.5571\n",
            "At 2700/5000: train loss is 1.2717 , val loss is 1.5550\n",
            "At 2800/5000: train loss is 1.2577 , val loss is 1.5607\n",
            "At 2900/5000: train loss is 1.2520 , val loss is 1.5630\n",
            "At 3000/5000: train loss is 1.2472 , val loss is 1.5529\n",
            "At 3100/5000: train loss is 1.2388 , val loss is 1.5603\n",
            "At 3200/5000: train loss is 1.2290 , val loss is 1.5596\n",
            "At 3300/5000: train loss is 1.2210 , val loss is 1.5603\n",
            "At 3400/5000: train loss is 1.2088 , val loss is 1.5594\n",
            "At 3500/5000: train loss is 1.2044 , val loss is 1.5622\n",
            "At 3600/5000: train loss is 1.1945 , val loss is 1.5672\n",
            "At 3700/5000: train loss is 1.1914 , val loss is 1.5640\n",
            "At 3800/5000: train loss is 1.1828 , val loss is 1.5667\n",
            "At 3900/5000: train loss is 1.1703 , val loss is 1.5755\n",
            "At 4000/5000: train loss is 1.1702 , val loss is 1.5804\n",
            "At 4100/5000: train loss is 1.1636 , val loss is 1.5768\n",
            "At 4200/5000: train loss is 1.1483 , val loss is 1.5825\n",
            "At 4300/5000: train loss is 1.1386 , val loss is 1.5803\n",
            "At 4400/5000: train loss is 1.1322 , val loss is 1.5940\n",
            "At 4500/5000: train loss is 1.1269 , val loss is 1.5998\n",
            "At 4600/5000: train loss is 1.1174 , val loss is 1.6056\n",
            "At 4700/5000: train loss is 1.1086 , val loss is 1.6133\n",
            "At 4800/5000: train loss is 1.1032 , val loss is 1.6163\n",
            "At 4900/5000: train loss is 1.0913 , val loss is 1.6280\n",
            "At 4999/5000: train loss is 1.0809 , val loss is 1.6273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Characters"
      ],
      "metadata": {
        "id": "LnJ-WxmrEGGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros( (1,1),dtype=torch.long, device=device)\n",
        "idx = model.generate(context,max_new_tokens=2000)[0] # similar to flat tensor\n",
        "print(decode(idx.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AfpUAOxECtM",
        "outputId": "566b1ae1-4538-432f-8262-c32901b11b9f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KING EDWARD IV:\n",
            "Ah, would your hinder love to tears:\n",
            "So we he will be my cousin pass'd me against\n",
            "Crying hostes and eat not another head,\n",
            "Shall soon ourselves are fortune fortune;\n",
            "Our scope of thine, the house of itself,\n",
            "Setting on them and raven! O woe!\n",
            "Thy day fall is weab the father. Farewell.\n",
            "\n",
            "PRINCE:\n",
            "God about thy business.\n",
            "\n",
            "CORIOLANUS:\n",
            "Men gentlemen such vineyard of the minim,\n",
            "That will it more stand tear news with my rock.\n",
            "And with thy brother tomb, had he for Tybbodd's\n",
            "Exeter, impect the belly, then\n",
            "supplus fear this gentleman, which I intend\n",
            "Should disciple myself and vengeance\n",
            "And so learned the golden serving how\n",
            "Each one the boody blessed, and yet it is my life,\n",
            "good man, be so he'med to see me to your babe.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Ah, my lord; and leave out a sister,\n",
            "Nor bid me so gross a precious point that\n",
            "At a duke low capable. Ah, so grief, wherein he\n",
            "himself out, hodest he put misery.\n",
            "\n",
            "MAMILLIUS:\n",
            "The city are no remedy chits for you: you shall pay with\n",
            "thee here in weddman cold and air ancestors and tell him?\n",
            "\n",
            "CAMILLO:\n",
            "The grounded wife thy thousand sung clocks me to thee.\n",
            "He could, my lord Northumb and bubble\n",
            "And be a treater bear of the justice: is the ost\n",
            "wrong compunion, my lord. for doubt, tell me fair,\n",
            "'Jove' may be prevented to me.'\n",
            "\n",
            "Provost: here's spake, and so blear our terment:\n",
            "but it must be accocatred woman in her.\n",
            "Say, Scroop, where thou my peace than fair,\n",
            "And my good queen he is here. Go you am I\n",
            "To that a while! but a ssa? and leave of those\n",
            "That would walk\n",
            "Against the Capulet,--hearing, poor appoint-brats--\n",
            "With roses of an ass bright a man impossible\n",
            "Than the externance of the gates, and hanging them,\n",
            "And favours starved some other end,\n",
            "The bed, and doth might that lick famed wish.\n",
            "But how to how my fowled with loss he swood sleep,\n",
            "I shall not seem to quick by some a monarous,\n",
            "His happy Forbia's fatal and desperate!\n",
            "Most gearated noble sir, and brothers' fathers!\n",
            "If our hope against upon my bosom first.\n",
            "\n",
            "BENVOLIO:\n",
            "A Roman, the battle; t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "p_Y-_VKkcm6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqP5nJ6weVjd",
        "outputId": "687bb46c-cd1e-4b06-d01d-c10d1f294191"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save weights\n",
        "path = F\"/content/gdrive/MyDrive/Colab Notebooks/trained_models/tiny_grad/nanogpt_v1.pt\"\n",
        "torch.save(model.state_dict(),path)"
      ],
      "metadata": {
        "id": "VfYYse1lSYxE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahTqdaN5fiq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}